# web-ai-model-proxy-cache
Retrieve large (GBs) AI binary model files from cloud, cache locally as sharded blobs to load faster on 2nd page load, returns stored file as data URL
